---
layout: page
title: About Me
---

<img src='{{site.baseurl}}/downloads/headshot.jpg' width='255' height='330'>

Welcome to my webpage!

I am a research scientist at Element AI that has just been acquired by ServiceNow. I am also a Core Industry Member of [Mila](https://mila.quebec/) and Adjunct Professor at [McGill University](https://www.mcgill.ca/).

I believe Human Language Techonologies (HLT: a better name than NLP) will change the way humans interact with software and access knowledge. In fact, this has already happened (think web search), but this is just the beginning. I am interested in research questions at all levels of the HLT techonology stack including fundamentals of deep learning, foundation model training, task-specific algorithms (especially semantic parsing), user experience with AI systems. Keyword-wise, my recent and on-going work focuses on semantic parsing and task-oriented dialogue methods, code generation, systematic (compositional) generalization and sample efficiency of neural models. 

My prior research interests include grounding language in vision and action, question answering, speech recognition, machine translation and structured prediction in general. 

I have did my PhD at Mila working under supervision of Yoshua Bengio. 

A bit of bragging: I invented the 
[content-based neural attention](https://arxiv.org/abs/1409.0473) that is now a core tool in deep-learning-based natural language processing.

### Research Highlights

- **Edge Transformer: a new neural architecture inspired by Prolog and Transformers**
  [Systematic Generalization with Edge Transformers](https://openreview.net/forum?id=UUds0Jr_XWk) <br>
    L. Bergen, T. J. O'Donnell, D. Bahdanau <br>
    EMNLP 2021

- **semantic parsing by labeling edges and nodes of an aligned graph** <br>
  [LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing
](https://arxiv.org/abs/2110.07572) <br>
    D. Jambor, D. Bahdanau <br>


- **state-of-the-art text-to-SQL with constrained inference** <br>
  [PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models
](https://arxiv.org/abs/2109.05093) <br>
    T. Scholak, N. Schucher, D. Bahdanau <br>
    EMNLP 2021


### Previous Work 

- **a simpler and faster text-to-SQL model** <br>
  [DuoRAT: Towards Simpler Text-to-SQL Models](https://arxiv.org/abs/2010.11119) <br>
    T. Scholak, R. Li, D. Bahdanau, H. de Vries, C. Pal <br>
    NAACL 2020

- **a position paper on ecological validity of existing datasets and benchmarks for language user interfaces** <br>
  [Towards Ecologically Valid Research on Language User Interfaces](https://arxiv.org/abs/2007.14435) <br>
    H. de Vries, D. Bahdanau, C. Manning <br>

- **a new systematic generalization test for visual question answering** <br>
  [CLOSURE: Assessing Systematic Generalization of CLEVR Models](https://arxiv.org/abs/1912.05783) <br>
    D. Bahdanau, H. de Vries, T. J. O'Donnell, S. Murty, P. Beaudoin, Y. Bengio, A. Courville <br>
    ArXiV

- **a study on sytematic generalization in modular visual question answering architectures** <br> 
  [Systematic Generalization: What Is Required and Can It Be Learned?](https://arxiv.org/abs/1811.12889) <br>
    D. Bahdanau, S. Murty, M. Noukhovitch, T. H. Nguyen, H. de Vries, A. Courville <br>
    ICLR 2019

- **new platform to study sample efficiency of different instruction-following approaches** <br>
  [BabyAI: First Steps Towards Grounded Language Learning With a Human In the Loop](https://arxiv.org/abs/1810.08272) <br>
    M. Chevalier-Boisvert, D. Bahdanau, S. Lahlou, L. Willems, C. Saharia, T.H. Nguyen, Y. Bengio <br>
    ICLR 2019

- **instruction-following with goal-states instead of complete demonstrations** <br>
  [Learning to Understand Goal Specifications by Modelling Reward](https://arxiv.org/pdf/1806.01946.pdf) <br>
    D. Bahdanau, F. Hill, J. Leike, E. Hughes, P. Kohli, E. Grefenstette  <br>
    ICLR 2019

- **training criteria for sequence prediction tasks** <br>
  [An Actor-Critic Algorithm for Sequence Prediction](https://arxiv.org/abs/1607.07086) <br>
    D. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe, J. Pineau, A. Courville, Y. Bengio <br>
    ICLR 2017

- **adapting recurrent networks with attention to do speech recognition** <br>
  [End-to-End Attention-based Large Vocabulary Speech Recognition](https://arxiv.org/abs/1508.04395) <br>
    D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, Y. Bengio <br>
    ICASSP 2016, oral

  [Attention-Based Methods for Speech Recognition](https://arxiv.org/abs/1506.07503) <br>
    J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, Y. Bengio <br>
    NIPS 2015, spotlight
- **deep learning software on top of Theano** <br>
  [Blocks and Fuel: frameworks for deep learning](http://arxiv.org/pdf/1506.00619) <br>
    B. MerriÃ«nboer, D, Bahdanau, V. Dumoulin, D. Warde-Farley, J. Chorowski, Y. Bengio <br>
    2015, technical report
- **neural machine translation** <br>
  [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) <br>
    D. Bahdanau, K. Cho, Y. Bengio <br>
    ICLR 2015, oral

[My publications at Google Scholar](https://scholar.google.de/citations?user=Nq0dVMcAAAAJ&hl=de&oi=ao)
